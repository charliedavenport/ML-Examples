{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "California housing data fetcher is broken -- using Boston housing data instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "housing = load_boston()\n",
    "m, n = housing.data.shape  # (m = # of instances; n = # of instances)\n",
    "#print('shape = ({},{})'.format(m, n))\n",
    "# add a bias feature (necessary for lin reg)\n",
    "housing_biased = np.c_[np.ones((m,1)), housing.data]\n",
    "#print('shape = ', housing_biased.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Normal Equation**\n",
    "\n",
    "Computes parameter vector, **$\\theta$**, in one go\n",
    "\n",
    "$\\hat\\theta = \\left(X^T\\cdot X\\right)^{-1}\\cdot X^T\\cdot y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.65391960e+01]\n",
      " [ -1.07211098e-01]\n",
      " [  4.63852249e-02]\n",
      " [  2.08310299e-02]\n",
      " [  2.68740368e+00]\n",
      " [ -1.78212509e+01]\n",
      " [  3.80236006e+00]\n",
      " [  7.74049258e-04]\n",
      " [ -1.47647619e+00]\n",
      " [  3.05743963e-01]\n",
      " [ -1.23250699e-02]\n",
      " [ -9.54248607e-01]\n",
      " [  9.38500185e-03]\n",
      " [ -5.25576472e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(housing_biased, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "# theta = inv(XT * X) * XT * y\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_val = theta.eval()\n",
    "    print(theta_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# **Batch Gradient Descent (BGD)**\n",
    "\n",
    "Find model parameters incrementally, starting with random values for **$\\theta$**. At each step, calculate the gradient of the cost function with respect to each model parameter, $\\theta_j$, denoted\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j}MSE(\\mathbf{\\theta}) = \\frac 2m \\sum_{i=1}^{m} \\left(\\mathbf{\\theta}^T \\cdot \\mathbf{x}^{(i)} - y^{(i)}\\right)x_j^{(i)}$\n",
    "\n",
    "Instead of computing each partial derivative individually, we compute them in one go to find the gradient vector, $\\nabla_\\theta MSE(\\theta)$ using the following equation:\n",
    "\n",
    "$\\nabla_\\theta MSE(\\theta) = \\begin{pmatrix} \\frac{\\partial}{\\partial \\theta_0}MSE(\\theta) \\\\ \\frac{\\partial}{\\partial \\theta_1}MSE(\\theta) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_n}MSE(\\theta) \\end{pmatrix} = \\frac2m \\mathbf{X}^T\\cdot (\\mathbf{X} \\cdot \\theta - \\mathbf{y}) $\n",
    "\n",
    "The gradient vector points \"uphill\", so we negate it since we are minimizing the cost function. We also have a learning rate hyperparameter, $\\eta$, used to scale the gradient vector at each learning step. So, our equation for each step is:\n",
    "\n",
    "$\\theta^{(next)} = \\theta - \\eta\\nabla_{\\theta}MSE(\\theta) $\n",
    "\n",
    "In the next cell, the gradient is computed manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  622.075\n",
      "Epoch 100 MSE =  89.3275\n",
      "Epoch 200 MSE =  74.5117\n",
      "Epoch 300 MSE =  72.305\n",
      "Epoch 400 MSE =  70.8024\n",
      "Epoch 500 MSE =  69.6332\n",
      "Epoch 600 MSE =  68.7169\n",
      "Epoch 700 MSE =  67.9959\n",
      "Epoch 800 MSE =  67.426\n",
      "Epoch 900 MSE =  66.9729\n",
      "Final MSE =  66.6135\n",
      "[[  0.2783739 ]\n",
      " [ -0.23561998]\n",
      " [  2.33036089]\n",
      " [  0.82568747]\n",
      " [ -0.28459436]\n",
      " [ -0.58693224]\n",
      " [  1.41693676]\n",
      " [  2.02212644]\n",
      " [ -0.54973865]\n",
      " [  0.49315774]\n",
      " [  9.5137291 ]\n",
      " [ -0.12049896]\n",
      " [ 23.73558617]\n",
      " [ -0.74865985]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01 # learning rate \n",
    "n_epochs = 1000\n",
    "\n",
    "housing_biased_norm = normalize(housing_biased)\n",
    "X = tf.constant(housing_biased_norm, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "# manually compute gradients\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - eta * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        if epoch == n_epochs-1:\n",
    "            print(\"Final MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    theta_best = theta.eval()\n",
    "    print(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGD using autodiff\n",
    "\n",
    "tf.gradients() function takes in an op (mse) and a list of variables (just theta in this case), and creates a list of ops (one per variable) to compute the gradients of the op with respect to each variable. The gradients node computes the gradient vector of MSE w.r.t. theta\n",
    "\n",
    "The end result is more or less the same as using the manually computed gradient, but this example uses a *very* simple cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  591.538\n",
      "Epoch 100 MSE =  86.8639\n",
      "Epoch 200 MSE =  73.0164\n",
      "Epoch 300 MSE =  71.0756\n",
      "Epoch 400 MSE =  69.7683\n",
      "Epoch 500 MSE =  68.7508\n",
      "Epoch 600 MSE =  67.9528\n",
      "Epoch 700 MSE =  67.3242\n",
      "Epoch 800 MSE =  66.8268\n",
      "Epoch 900 MSE =  66.4308\n",
      "Final MSE =  66.1161\n",
      "[[ -0.39421716]\n",
      " [  0.40667704]\n",
      " [  3.96623063]\n",
      " [  0.27636206]\n",
      " [ -0.17432758]\n",
      " [  0.70810735]\n",
      " [  0.15927659]\n",
      " [  1.14151692]\n",
      " [ -0.20456119]\n",
      " [ -0.48585832]\n",
      " [  9.48361588]\n",
      " [  1.5889343 ]\n",
      " [ 23.84345436]\n",
      " [ -1.0559268 ]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01 # learning rate \n",
    "n_epochs = 1000\n",
    "\n",
    "housing_biased_norm = normalize(housing_biased)\n",
    "X = tf.constant(housing_biased_norm, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "# This is where we use autodiff\n",
    "gradients = tf.gradients(mse, [theta])[0] \n",
    "training_op = tf.assign(theta, theta - eta * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        if epoch == n_epochs-1:\n",
    "            print(\"Final MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    theta_best = theta.eval() \n",
    "    print(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch GD\n",
    "\n",
    "Instead of computing the gradients using the entire training set, we use a small, random subset of instances called *mini-batches*. This provides a large performance speedup at the cost of accuracy. To accomplish this, we need to replace X and y at every iteration with a different mini-batch. Placeholder nodes accomplish this by outputting data at runtime, instead of performing some computation. (If no value is specified for a placeholder, you get an exception)\n",
    "\n",
    "When evaluating a node dependent on a placeholder, we pass in a feed_dict to the eval() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  325.135\n",
      "Epoch 100 MSE =  29.1659\n",
      "Epoch 200 MSE =  31.6684\n",
      "Epoch 300 MSE =  32.8169\n",
      "Epoch 400 MSE =  33.1949\n",
      "Epoch 500 MSE =  33.1788\n",
      "Epoch 600 MSE =  32.9741\n",
      "Epoch 700 MSE =  32.6856\n",
      "Epoch 800 MSE =  32.3656\n",
      "Epoch 900 MSE =  32.0389\n",
      "Final MSE =  31.7206\n",
      "[[ -0.30211103]\n",
      " [ -1.19279504]\n",
      " [ 12.65120602]\n",
      " [ -2.79090834]\n",
      " [ -0.23257811]\n",
      " [ -0.76165146]\n",
      " [  2.00952196]\n",
      " [ -4.61977196]\n",
      " [ -0.0421115 ]\n",
      " [  0.58151853]\n",
      " [  8.58374977]\n",
      " [ -0.16875364]\n",
      " [ 24.57528496]\n",
      " [ -5.29968739]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01 # learning rate \n",
    "n_epochs = 1000\n",
    "\n",
    "housing_biased_norm = normalize(housing_biased)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "# This is where we use autodiff\n",
    "gradients = tf.gradients(mse, [theta])[0] \n",
    "training_op = tf.assign(theta, theta - eta * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    start_ind = batch_index * batch_size\n",
    "    end_ind = start_ind + batch_size\n",
    "    if end_ind > m: end_ind = m # cannot go out of bounds\n",
    "    #print('({},{})'.format(start_ind, end_ind))\n",
    "    X_batch = housing_biased_norm.data[start_ind:end_ind]\n",
    "    y_batch = housing.target.reshape(-1,1)[start_ind:end_ind]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        if epoch == n_epochs-1:\n",
    "            print(\"Final MSE = \", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "    theta_best = theta.eval() # no need for feed_dict, since theta does not depend on X or y\n",
    "    print(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
